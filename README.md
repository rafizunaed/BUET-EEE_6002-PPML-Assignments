# BUET-EEE_6002-PPML-Assignments
This repository contains the code files and reports for the assignments for EEE 6002 Privacy-Preserving Machine Learning Course conducted in the April 2020 semester.

# Logistic Regression
The code for Logistic Regression has been implemented from scratch.

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;sigmoid&space;\&space;function,&space;\&space;\sigma(z)=\frac{1}{1&plus;\exp^{-z}}&space;\\&space;&&space;hypothesis,&space;\&space;\hat{y}(\omega,&space;X)=\sigma(\omega&space;X)&space;\\&space;&&space;loss,&space;\&space;L(y,\hat{y},\omega)&space;=&space;-\bigm[\frac{1}{m}\sum_{i=1}^{m}(y_{i}\log(\hat{y}_{i})&plus;(1-y_{i})\log(1-\hat{y}_{i}))&space;-&space;\frac{\lambda}{2m}\sum_{i=1}^{n}||\omega_{i}||^2_2&space;\bigm]&space;\\&space;&&space;gradient,&space;\&space;\triangledown_{\omega}L&space;=&space;\frac{1}{m}(X^{T}(\hat{y}-y)&plus;\lambda\omega)&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha\triangledown_{\omega}L&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\begin{align*}&space;&&space;sigmoid&space;\&space;function,&space;\&space;\sigma(z)=\frac{1}{1&plus;\exp^{-z}}&space;\\&space;&&space;hypothesis,&space;\&space;\hat{y}(\omega,&space;X)=\sigma(\omega&space;X)&space;\\&space;&&space;loss,&space;\&space;L(y,\hat{y},\omega)&space;=&space;-\bigm[\frac{1}{m}\sum_{i=1}^{m}(y_{i}\log(\hat{y}_{i})&plus;(1-y_{i})\log(1-\hat{y}_{i}))&space;-&space;\frac{\lambda}{2m}\sum_{i=1}^{n}||\omega_{i}||^2_2&space;\bigm]&space;\\&space;&&space;gradient,&space;\&space;\triangledown_{\omega}L&space;=&space;\frac{1}{m}(X^{T}(\hat{y}-y)&plus;\lambda\omega)&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha\triangledown_{\omega}L&space;\end{align*}" title="\begin{align*} & sigmoid \ function, \ \sigma(z)=\frac{1}{1+\exp^{-z}} \\ & hypothesis, \ \hat{y}(\omega, X)=\sigma(\omega X) \\ & loss, \ L(y,\hat{y},\omega) = -\bigm[\frac{1}{m}\sum_{i=1}^{m}(y_{i}\log(\hat{y}_{i})+(1-y_{i})\log(1-\hat{y}_{i})) - \frac{\lambda}{2m}\sum_{i=1}^{n}||\omega_{i}||^2_2 \bigm] \\ & gradient, \ \triangledown_{\omega}L = \frac{1}{m}(X^{T}(\hat{y}-y)+\lambda\omega) \\ & weight \ update, \ \omega = \omega - \alpha\triangledown_{\omega}L \end{align*}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;sensitivity,&space;\&space;\Delta_{f}=\frac{2}{m}&space;\\&space;&&space;gaussian&space;\&space;noise,&space;f&space;=&space;N(0,&space;\frac{2\Delta_{f}^{2}}{\epsilon^{2}}\log(1.25/\delta))&space;\\&space;&&space;new&space;\&space;gradient,&space;\&space;(\triangledown_{\omega}L)_{new}&space;=&space;\triangledown_{\omega}L&space;&plus;&space;f&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha(\triangledown_{\omega}L)_{new}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\begin{align*}&space;&&space;sensitivity,&space;\&space;\Delta_{f}=\frac{2}{m}&space;\\&space;&&space;gaussian&space;\&space;noise,&space;f&space;=&space;N(0,&space;\frac{2\Delta_{f}^{2}}{\epsilon^{2}}\log(1.25/\delta))&space;\\&space;&&space;new&space;\&space;gradient,&space;\&space;(\triangledown_{\omega}L)_{new}&space;=&space;\triangledown_{\omega}L&space;&plus;&space;f&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha(\triangledown_{\omega}L)_{new}&space;\end{align*}" title="\begin{align*} & sensitivity, \ \Delta_{f}=\frac{2}{m} \\ & gaussian \ noise, f = N(0, \frac{2\Delta_{f}^{2}}{\epsilon^{2}}\log(1.25/\delta)) \\ & new \ gradient, \ (\triangledown_{\omega}L)_{new} = \triangledown_{\omega}L + f \\ & weight \ update, \ \omega = \omega - \alpha(\triangledown_{\omega}L)_{new} \end{align*}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;sensitivity,&space;\&space;\Delta_{f}=\frac{2}{m}&space;\\&space;&&space;beta,&space;\beta&space;=&space;\frac{\Delta_{f}}{\epsilon}&space;\\&space;&&space;laplacian&space;\&space;noise,&space;f&space;=&space;L_{p}(0,&space;beta)&space;\\&space;&&space;new&space;\&space;gradient,&space;\&space;(\triangledown_{\omega}L)_{new}&space;=&space;\triangledown_{\omega}L&space;&plus;&space;f&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha(\triangledown_{\omega}L)_{new}&space;%\\&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\begin{align*}&space;&&space;sensitivity,&space;\&space;\Delta_{f}=\frac{2}{m}&space;\\&space;&&space;beta,&space;\beta&space;=&space;\frac{\Delta_{f}}{\epsilon}&space;\\&space;&&space;laplacian&space;\&space;noise,&space;f&space;=&space;L_{p}(0,&space;beta)&space;\\&space;&&space;new&space;\&space;gradient,&space;\&space;(\triangledown_{\omega}L)_{new}&space;=&space;\triangledown_{\omega}L&space;&plus;&space;f&space;\\&space;&&space;weight&space;\&space;update,&space;\&space;\omega&space;=&space;\omega&space;-&space;\alpha(\triangledown_{\omega}L)_{new}&space;%\\&space;\end{align*}" title="\begin{align*} & sensitivity, \ \Delta_{f}=\frac{2}{m} \\ & beta, \beta = \frac{\Delta_{f}}{\epsilon} \\ & laplacian \ noise, f = L_{p}(0, beta) \\ & new \ gradient, \ (\triangledown_{\omega}L)_{new} = \triangledown_{\omega}L + f \\ & weight \ update, \ \omega = \omega - \alpha(\triangledown_{\omega}L)_{new} %\\ \end{align*}" /></a>
